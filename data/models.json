[
  {
    "id": "llama-3-1",
    "name": "LLaMA 3.1",
    "releaseDate": "2024-07-23",
    "parameters": [8, 70, 405],
    "license": "Meta Llama 3.2 Community License",
    "description": "Meta's latest iteration in the LLaMA series, offering models with up to 405 billion parameters, primarily focusing on text generation and coding support.",
    "category": "text",
    "tags": ["text-generation", "coding", "reasoning"],
    "image": "/images/logo-llama_2.png",
    "featured": true
  },
  {
    "id": "deepseek-r1",
    "name": "DeepSeek-R1",
    "releaseDate": "2025-01-20",
    "parameters": [671],
    "license": "MIT License",
    "description": "Developed by DeepSeek, emphasizing reasoning capabilities, trained at a cost of approximately $5.6 million, and recognized for its efficiency and performance.",
    "category": "text",
    "tags": ["reasoning", "efficiency", "performance"],
    "image": "/images/logo_deepseek.png",
    "featured": true
  },
  {
    "id": "mistral-7b",
    "name": "Mistral 7B",
    "releaseDate": "2023-09-27",
    "parameters": [7.3],
    "license": "Apache 2.0",
    "description": "A 7.3 billion parameter language model using the transformers architecture, known for its efficiency and scalability.",
    "category": "text",
    "tags": ["text-generation", "efficiency", "scalability"],
    "image": "/images/logo_mistral.png",
    "featured": true
  },
  {
    "id": "mixtral-8x7b",
    "name": "Mixtral 8x7B",
    "releaseDate": "2023-12-09",
    "parameters": [46.7],
    "license": "Apache 2.0",
    "description": "Utilizes a sparse mixture of experts architecture, outperforming models like LLaMA 70B and GPT-3.5 in most benchmarks.",
    "category": "text",
    "tags": ["text-generation", "mixture-of-experts", "high-performance"],
    "image": "/images/logo_mistral.png",
    "featured": false
  },
  {
    "id": "flux",
    "name": "Flux",
    "releaseDate": "2024-08-15",
    "parameters": [12],
    "license": "Apache License (Schnell), Non-commercial license (Dev), Proprietary (Pro)",
    "description": "Developed by Black Forest Labs, Flux is a text-to-image model based on a hybrid architecture combining multimodal and parallel diffusion transformer blocks.",
    "category": "image",
    "tags": ["text-to-image", "diffusion", "multimodal"],
    "image": "/images/logo_black-forest.png",
    "featured": true
  },
  {
    "id": "stable-diffusion-3",
    "name": "Stable Diffusion 3",
    "releaseDate": "2024-03-15",
    "parameters": [8],
    "license": "CreativeML Open RAIL++-M License",
    "description": "Stability AI's third-generation text-to-image model, featuring improved image quality, better text understanding, and enhanced compositional capabilities.",
    "category": "image",
    "tags": ["text-to-image", "diffusion", "photorealistic"],
    "image": "/images/logo_stability_ai.png",
    "featured": false
  },
  {
    "id": "dbrx",
    "name": "DBRX",
    "releaseDate": "2024-03-27",
    "parameters": [132],
    "license": "Databricks Open License",
    "description": "A mixture-of-experts transformer model that outperforms other prominent open-source models in language understanding, programming ability, and mathematics.",
    "category": "text",
    "tags": ["language-understanding", "programming", "mathematics"],
    "image": "/images/logo_dbrx.png",
    "featured": false
  },
  {
    "id": "qwen-2-5",
    "name": "Qwen 2.5",
    "releaseDate": "2024-05-10",
    "parameters": [72],
    "license": "Apache 2.0",
    "description": "Developed by Alibaba Cloud, Qwen 2.5 is proficient in mathematics, coding, and supports over 29 languages, catering to a wide array of AI applications.",
    "category": "text",
    "tags": ["multilingual", "coding", "mathematics"],
    "image": "/images/logo_qwen.png",
    "featured": false
  },
  {
    "id": "gpt-j",
    "name": "GPT-J",
    "releaseDate": "2021-03-15",
    "parameters": [6],
    "license": "Apache 2.0",
    "description": "A 6 billion parameter language model developed by EleutherAI, serving as a free alternative to OpenAI's GPT-3.",
    "category": "text",
    "tags": ["text-generation", "open-source", "gpt-alternative"],
    "image": "/images/logo_eleutherai.png",
    "featured": false
  },
  {
    "id": "pythia",
    "name": "Pythia",
    "releaseDate": "2023-10-15",
    "parameters": [13],
    "license": "Apache 2.0",
    "description": "A suite designed to facilitate scientific research on the capabilities of large language models, featuring 154 partially trained model checkpoints.",
    "category": "text",
    "tags": ["research", "language-models", "training-analysis"],
    "image": "/images/logo_pythia.png",
    "featured": false
  },
  {
    "id": "falcon",
    "name": "Falcon",
    "releaseDate": "2023-06-15",
    "parameters": [40],
    "license": "Apache 2.0",
    "description": "Developed by the Advanced Technology Research Council in Abu Dhabi, Falcon models surpassed those from Meta and Google at the time of release.",
    "category": "text",
    "tags": ["text-generation", "multilingual", "high-performance"],
    "image": "/images/logo_falcon.png",
    "featured": false
  },
  {
    "id": "llava-1-6",
    "name": "LLaVA 1.6",
    "releaseDate": "2024-03-20",
    "parameters": [13],
    "license": "Apache 2.0",
    "description": "An open-source multimodal model that combines vision and language capabilities, built on top of Vicuna and CLIP models.",
    "category": "multimodal",
    "tags": ["vision", "language", "open-source"],
    "image": "/images/logo_llava.png",
    "featured": false
  },
  {
    "id": "cogvlm",
    "name": "CogVLM",
    "releaseDate": "2023-10-10",
    "parameters": [17],
    "license": "Apache 2.0",
    "description": "A vision-language model developed by THUDM that excels at complex visual reasoning tasks and detailed image understanding.",
    "category": "multimodal",
    "tags": ["vision", "reasoning", "image-understanding"],
    "image": "/images/logo_cogvlm.png",
    "featured": false
  },
  {
    "id": "moondream",
    "name": "Moondream",
    "releaseDate": "2024-02-15",
    "parameters": [1.6],
    "license": "Apache 2.0",
    "description": "A lightweight vision-language model designed to run efficiently on consumer hardware while providing strong visual understanding capabilities.",
    "category": "multimodal",
    "tags": ["vision", "efficient", "lightweight"],
    "image": "/images/logo_moondream.png",
    "featured": false
  },
  {
    "id": "videopoet",
    "name": "VideoPoet",
    "releaseDate": "2023-12-15",
    "parameters": [5],
    "license": "Research License (Non-commercial)",
    "description": "Google's open research model for text-to-video generation with capabilities for video stylization, editing, and animation.",
    "category": "video",
    "tags": ["text-to-video", "stylization", "editing"],
    "image": "/images/logo_google.png",
    "featured": false
  },
  {
    "id": "modelscope-t2v",
    "name": "ModelScope T2V",
    "releaseDate": "2023-11-10",
    "parameters": [3],
    "license": "Apache 2.0",
    "description": "An open-source text-to-video generation model from Alibaba that can create short video clips from text descriptions.",
    "category": "video",
    "tags": ["text-to-video", "generation", "open-source"],
    "image": "/images/logo_modelscope.png",
    "featured": false
  },
  {
    "id": "whisper-large-v3",
    "name": "Whisper Large v3",
    "releaseDate": "2024-04-10",
    "parameters": [1.5],
    "license": "MIT License",
    "description": "OpenAI's speech recognition model capable of transcribing and translating audio in multiple languages with high accuracy.",
    "category": "audio",
    "tags": ["speech-recognition", "transcription", "multilingual"],
    "image": "/images/logo_openai.png",
    "featured": false
  },
  {
    "id": "musicgen",
    "name": "MusicGen",
    "releaseDate": "2023-06-10",
    "parameters": [3.3],
    "license": "Meta Research License",
    "description": "Meta's text-to-music generation model capable of creating high-quality music samples from text descriptions.",
    "category": "audio",
    "tags": ["text-to-music", "generation", "creative"],
    "image": "/images/logo-llama_2.png",
    "featured": false
  },
  {
    "id": "bark",
    "name": "Bark",
    "releaseDate": "2023-04-15",
    "parameters": [2.9],
    "license": "MIT License",
    "description": "A transformer-based text-to-audio model from Suno that can generate realistic speech, music, and sound effects.",
    "category": "audio",
    "tags": ["text-to-audio", "speech-synthesis", "sound-effects"],
    "image": "/images/logo_suno.png",
    "featured": false
  },
  {
    "id": "idefics-2",
    "name": "IDEFICS 2",
    "releaseDate": "2024-05-15",
    "parameters": [8, 80],
    "license": "Apache 2.0",
    "description": "Hugging Face's open-source multimodal model that can understand and generate text based on images, with strong performance on visual reasoning tasks.",
    "category": "multimodal",
    "tags": ["vision", "language", "open-source"],
    "image": "/images/logo_huggingface.png",
    "featured": false
  },
  {
    "id": "qwen-vl",
    "name": "Qwen-VL",
    "releaseDate": "2024-02-20",
    "parameters": [7, 18],
    "license": "Apache 2.0",
    "description": "Alibaba's vision-language model that supports image understanding, visual question answering, and OCR capabilities in multiple languages.",
    "category": "multimodal",
    "tags": ["vision", "multilingual", "OCR"],
    "image": "/images/logo_qwen.png",
    "featured": false
  },
  {
    "id": "fuyu-8b",
    "name": "Fuyu-8B",
    "releaseDate": "2023-10-20",
    "parameters": [8],
    "license": "Adept Model License",
    "description": "Adept's multimodal model designed for document understanding, with strong capabilities for processing text within images and diagrams.",
    "category": "multimodal",
    "tags": ["document-understanding", "vision", "OCR"],
    "image": "/images/logo_adept.png",
    "featured": false
  },
  {
    "id": "gemma-1-1-vision",
    "name": "Gemma 1.1 Vision",
    "releaseDate": "2024-06-10",
    "parameters": [9],
    "license": "Gemma License (Open)",
    "description": "Google's open-source multimodal model that combines vision and language capabilities, designed for image understanding and visual reasoning tasks.",
    "category": "multimodal",
    "tags": ["vision", "language", "open-source"],
    "image": "/images/logo_google.png",
    "featured": false
  },
  {
    "id": "phi-3-vision",
    "name": "Phi-3 Vision",
    "releaseDate": "2024-05-20",
    "parameters": [4],
    "license": "Microsoft Research License",
    "description": "Microsoft's compact multimodal model that offers strong vision-language capabilities while being efficient enough to run on consumer hardware.",
    "category": "multimodal",
    "tags": ["vision", "efficient", "compact"],
    "image": "/images/logo_microsoft.png",
    "featured": false
  }
] 