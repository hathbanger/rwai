[
  {
    "id": "rtx-5090",
    "name": "GeForce RTX 5090 PNY",
    "manufacturer": "NVIDIA",
    "architecture": "Blackwell",
    "specs": {
      "cudaCores": 21760,
      "tensorCores": 680,
      "rayTracingCores": 170,
      "baseClockSpeed": "2.01 GHz",
      "boostClockSpeed": "2.41 GHz",
      "memoryType": "GDDR7",
      "memorySize": "32 GB",
      "memoryBusWidth": "512-bit",
      "memoryBandwidth": "1,792 GB/s",
      "transistorCount": "92 billion",
      "dieSize": "750 mm²",
      "tdp": "575 W"
    },
    "launchDate": "2025-01-30",
    "launchMsrp": "$1,999",
    "keyFeatures": [
      "GDDR7 memory",
      "PCIe 5.0 interface",
      "DisplayPort 2.1b connectors supporting up to 8K at 165Hz",
      "DLSS 4 technology with Multi Frame Generation and AI enhancements"
    ],
    "performance": "Offers twice the performance of the RTX 4090",
    "description": "NVIDIA's flagship consumer GPU, targeting gamers and content creators, featuring advanced ray tracing and AI capabilities.",
    "category": "consumer",
    "tags": ["gaming", "content-creation", "ray-tracing", "AI"],
    "image": "/images/GPU_RTX-5090-PNY.png",
    "featured": true
  },
  {
    "id": "h-100",
    "name": "H100 SGX",
    "manufacturer": "NVIDIA",
    "architecture": "Hopper",
    "specs": {
      "cudaCores": 14592,
      "tensorCores": 456,
      "rayTracingCores": "N/A",
      "baseClockSpeed": "1.095 GHz",
      "boostClockSpeed": "1.755 GHz",
      "memoryType": "HBM2e",
      "memorySize": "80 GB",
      "memoryBusWidth": "5120-bit",
      "memoryBandwidth": "2,040 GB/s",
      "transistorCount": "80 billion",
      "dieSize": "814 mm²",
      "tdp": "350 W"
    },
    "launchDate": "2023-03-21",
    "launchMsrp": "$30,000",
    "keyFeatures": [
      "HBM2e memory",
      "PCIe 5.0 interface",
      "NVLink support",
      "4th generation Tensor Cores",
      "Transformer Engine with FP8 precision"
    ],
    "performance": "Delivers up to 3X higher performance for AI training and up to 30X higher performance for AI inference compared to the previous generation.",
    "description": "NVIDIA's data center GPU designed for high-performance computing and AI workloads, featuring exceptional performance for large language models and deep learning applications.",
    "category": "datacenter",
    "tags": ["HPC", "AI", "data-center", "enterprise", "LLM"],
    "image": "/images/GPU_H100-SGX.png",
    "featured": true
  },
  {
    "id": "h-200",
    "name": "H200 SGX",
    "manufacturer": "NVIDIA",
    "architecture": "Hopper",
    "specs": {
      "cudaCores": 17000,
      "tensorCores": 680,
      "rayTracingCores": "N/A",
      "baseClockSpeed": "1.8 GHz",
      "boostClockSpeed": "2.3 GHz",
      "memoryType": "HBM3",
      "memorySize": "80 GB",
      "memoryBusWidth": "5120-bit",
      "memoryBandwidth": "3,276 GB/s",
      "transistorCount": "80 billion",
      "dieSize": "814 mm²",
      "tdp": "700 W"
    },
    "launchDate": "2024-11-01",
    "launchMsrp": "$10,000",
    "keyFeatures": [
      "HBM3 memory",
      "PCIe 5.0 interface",
      "NVLink support",
      "Enhanced AI and HPC capabilities"
    ],
    "performance": "Designed for high-performance computing and AI workloads, delivering significant performance improvements over its predecessor, the H-100.",
    "description": "Part of NVIDIA's data center lineup, optimized for high-performance computing (HPC) and artificial intelligence (AI) workloads, offering substantial memory and computational power.",
    "category": "datacenter",
    "tags": ["HPC", "AI", "data-center", "enterprise"],
    "image": "/images/GPU_H200-SGX.png",
    "featured": true
  },
  {
    "id": "gb200-nvl4",
    "name": "GB200 NVL4",
    "manufacturer": "NVIDIA",
    "architecture": "Blackwell",
    "specs": {
      "cudaCores": 18500,
      "tensorCores": 720,
      "rayTracingCores": "N/A",
      "baseClockSpeed": "1.9 GHz",
      "boostClockSpeed": "2.5 GHz",
      "memoryType": "HBM3e",
      "memorySize": "192 GB",
      "memoryBusWidth": "6144-bit",
      "memoryBandwidth": "8,000 GB/s",
      "transistorCount": "208 billion",
      "dieSize": "850 mm²",
      "tdp": "1,200 W"
    },
    "launchDate": "2024-03-18",
    "launchMsrp": "$40,000",
    "keyFeatures": [
      "HBM3e memory with unprecedented bandwidth",
      "5th generation NVLink: 1.8TB/s",
      "PCIe Gen6: 256GB/s",
      "Second-generation Transformer Engine with FP4/FP8 precision",
      "Dedicated decompression engines",
      "Multi-Instance GPU (MIG) support"
    ],
    "performance": "Delivers 30X faster real-time LLM inference performance for trillion-parameter language models compared to H100, and 4X faster training for large language models at scale.",
    "description": "NVIDIA's revolutionary Grace Blackwell Superchip that unlocks the future of converged HPC and AI, delivering exceptional performance through four NVIDIA NVLink-connected Blackwell GPUs unified with two Grace CPUs over NVLink-C2C.",
    "category": "datacenter",
    "tags": ["HPC", "AI", "data-center", "enterprise", "LLM", "supercomputing"],
    "image": "/images/GPU_gb200-nvl4.png",
    "featured": true
  }
] 